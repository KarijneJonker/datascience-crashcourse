{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OpenML and Scikit-Learn Tutorial \n",
    "\n",
    "This notebook covers key concepts of OpenML and simple examples of its use in combination with Python and Scikit-Learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenML is an online collaboration platform for machine learning which allows\n",
    "you to:\n",
    "\n",
    "* Find or share interesting, well-documented datasets\n",
    "* Define research / modelling goals (tasks)\n",
    "* Explore large amounts of machine learning algorithms, with APIs in Java, R, Python\n",
    "* Log and share reproducible experiments, models, results\n",
    "* Works seamlessly with scikit-learn and other libraries\n",
    "* Large scale benchmarking, compare to state of the art\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "\n",
    "## Installation\n",
    "Installation is done either through *Anaconda-Navigator* (see *Environments*), or via ``pip``:\n",
    "\n",
    ".. code:: bash\n",
    "\n",
    "    pip install openml scikit-learn\n",
    "\n",
    "For further information, please check out the installation guide at https://openml.github.io/openml-python/develop/usage.html#installation and https://scikit-learn.org/stable/install.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD 3-Clause\n",
    "\n",
    "import openml\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Authentication with main server\n",
    "\n",
    "The OpenML main server can only be accessed by users who have signed up on the\n",
    "OpenML platform. If you donâ€™t have an account yet, sign up now.\n",
    "You will receive an *API key*, which will authenticate you to the server\n",
    "and allow you to download and upload datasets, tasks, runs and flows.\n",
    "\n",
    "It is important to configure the Python connector with the proper API endpoint (usually good by default) and the proper API key. \n",
    "\n",
    "* Create an OpenML account (free) on https://www.openml.org.\n",
    "* After logging in, open your account page (avatar on the top right)\n",
    "* Open 'Account Settings', then 'API authentication' to find your API key.\n",
    "\n",
    "There are several ways to permanently authenticate:\n",
    "\n",
    "* Create a plain text file **~/.openml/config** with the line\n",
    "  **'apikey=MYKEY'**, replacing **MYKEY** with your API key. The config\n",
    "  file must be in the directory ~/.openml/config and exist prior to\n",
    "  importing the openml module. \n",
    "*  Running the code below and replacing 'MYKEY' with your API key, you authenticate for the duration of the python process. \n",
    "* Use the ``openml`` CLI tool with ``openml configure apikey MYKEY``,\n",
    "  replacing **MYKEY** with your API key.\n",
    "\n",
    "**IMPORTANT: Do not share code with credentials such as your OpenML API key**\n",
    "For example, in the first option above, keep your API key in a configuration file that is not synchonised through your Git-repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "\n",
    "openml.config.server = 'https://test.openml.org/api/v1/'\n",
    "openml.config.apikey = 'MYKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Test server\n",
    "\n",
    "You can specify to use a test server for OpenML, rather than the main server. \n",
    "For example, when you run code that simulates the upload of data. In such an example, you can connect to the test server at test.openml.org. This prevents the main server from crowding with example datasets, tasks, runs, and so on. The use of this test server can affect behaviour and performance of the OpenML-Python API.\n",
    "\n",
    "Before first connecting to the test server, specify:\n",
    "\n",
    "    openml.config.start_using_configuration_for_example()\n",
    "\n",
    "\n",
    "After completing all connections, specify:\n",
    "\n",
    "    openml.config.stop_using_configuration_for_example()\n",
    "\n",
    "\n",
    "When using the main server instead, make sure your apikey is configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "\n",
    "openml.config.start_using_configuration_for_example()\n",
    "# ... some code that connects to the server ...\n",
    "openml.config.stop_using_configuration_for_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "When downloading datasets, tasks, runs and flows, they will be cached to\n",
    "retrieve them without calling the server later. As with the API key,\n",
    "the cache directory can be either specified through the config file or\n",
    "through the API:\n",
    "\n",
    "* Add the  line **cachedir = 'MYDIR'** to the config file, replacing\n",
    "  'MYDIR' with the path to the cache directory. By default, OpenML\n",
    "  will use **~/.openml/cache** as the cache directory.\n",
    "* Run the code below, replacing 'MYDIR' with the path to the cache directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your OpenML cache directory\n",
    "# import os\n",
    "# openml.config.cache_directory = os.path.expanduser('MYDIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets, tasks, and runs\n",
    "\n",
    "OpenML distinguishes datasets, tasks, flows, and runs. This structure facilitates sharing of results of experiments with different model (configurations).\n",
    "\n",
    "- **Datasets** simply consist of a number of rows, also called instances, usually in tabular form. They are identified by their *did*.\n",
    "  Example: https://www.openml.org/search?type=data&sort=runs&id=40704\n",
    "  Note: There are often multiple variants of one dataset. For example, compare the results of a search for all datasets that contain 'Titanic': https://www.openml.org/search?type=data&status=active\n",
    "  \n",
    "-  **Tasks** consists of a dataset, together with a machine learning task to perform, such as classification or clustering and an evaluation method. For supervised tasks, this also specifies the target column in the data.\n",
    "  Example: https://www.openml.org/search?type=task&id=146230&source_data.data_id=40704\n",
    "\n",
    "- **Flows** identify a particular machine learning algorithm from a particular library or framework such as scikit-learn, Weka, or mlr. It should at least contain a name, details about the workbench and its version and a list of settable hyperparameters. Ideally, the appropriate workbench can deserialize it again (the algorithm, not the model).\n",
    "  Example: https://www.openml.org/search?type=flow&id=18869\n",
    "\n",
    "- **Runs** are particular flows, that is algorithm, with a particular parameter setting, applied to a particular task.\n",
    "  Example: https://www.openml.org/search?type=run&id=10229128&run_task.task_id=146230\n",
    "\n",
    "\n",
    "OpenML ensures that all datasets are uniformy formatted. They have rich, consistent metadata that is automatically managed and allows to filter datasets. This is important in particular for datasets that might have a large number of instances and variables.\n",
    "\n",
    "For more information, see https://docs.openml.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example\n",
    "\n",
    "\n",
    "**Example-1**: In the examples below, we will use the same task from the simple example above (task `146230` https://www.openml.org/search?type=task&sort=runs&id=146230 for the `Titanic` dataset with did `40704`: https://www.openml.org/search?type=data&sort=runs&id=40704&status=active), but with optimised classifiers. \n",
    "\n",
    "**Example-2 (Bonus)**: the more complex classification task `167119`  https://www.openml.org/search?type=task&sort=runs&id=167119 for the `Jungle Chess` dataset with did `41027`: https://www.openml.org/search?type=data&sort=runs&id=41027&status=active\n",
    "\n",
    "\n",
    "Download the OpenML task 146230 for the Titanic dataset, and execute it with a kNN classifier. Report the results in accuracy and training runtime.\n",
    "\n",
    "\n",
    "\n",
    "- Get the task and dataset using\n",
    "\n",
    "      openml.tasks.get_task(tid)\n",
    "      openml.datasets.get_dataset(did)\n",
    "      \n",
    "- Run the model of a KNeighborsClassifier on that task using\n",
    "\n",
    "      openml.runs.run_model_on_task(clf, task)\n",
    "\n",
    "- Print a summary of the run using\n",
    "\n",
    "      print(run)\n",
    "           \n",
    "- State the accuracy and runtime on each of the first (bonus: all 10) fold(s) using\n",
    "      \n",
    "      measures[\"predictive_accuracy\"][repeat][fold]\n",
    "      measures[\"wall_clock_time_millis_training\"][repeat][fold]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple OpenML Example:\n",
      "\n",
      "Experiment started.\n",
      "Experiment completed.\n",
      "\n",
      "OpenML Run\n",
      "==========\n",
      "Uploader Name: None\n",
      "Metric.......: None\n",
      "Run ID.......: None\n",
      "Task ID......: 167119\n",
      "Task Type....: None\n",
      "Task URL.....: https://www.openml.org/t/167119\n",
      "Flow ID......: None\n",
      "Flow Name....: sklearn.neighbors._classification.KNeighborsClassifier\n",
      "Flow URL.....: https://www.openml.org/f/None\n",
      "Setup ID.....: None\n",
      "Setup String.: Python_3.9.12. Sklearn_1.0.2. NumPy_1.21.2. SciPy_1.7.3.\n",
      "Dataset ID...: 41027\n",
      "Dataset URL..: https://www.openml.org/d/41027\n",
      "\n",
      "The timing and performance metrics available: \n",
      "usercpu_time_millis_training\n",
      "wall_clock_time_millis_training\n",
      "usercpu_time_millis_testing\n",
      "usercpu_time_millis\n",
      "wall_clock_time_millis_testing\n",
      "wall_clock_time_millis\n",
      "predictive_accuracy\n",
      "\n",
      "For the first repetition and fold, accuracy is 0.7361, training wall time is 129.1437\n",
      "\n",
      "The results over all repetitions and folds, \n",
      "with accuracy (`predictive_accuracy`) \n",
      "and training time(wall_clock_time_millis_training):\n",
      "Repeat #0, Fold #0: Accuracy: 0.7361, Wall-Time: 129.1437\n",
      "Repeat #0, Fold #1: Accuracy: 0.7390, Wall-Time: 49.0513\n",
      "Repeat #0, Fold #2: Accuracy: 0.7381, Wall-Time: 52.3098\n",
      "Repeat #0, Fold #3: Accuracy: 0.7492, Wall-Time: 55.7485\n",
      "Repeat #0, Fold #4: Accuracy: 0.7485, Wall-Time: 49.0277\n",
      "Repeat #0, Fold #5: Accuracy: 0.7474, Wall-Time: 51.5115\n",
      "Repeat #0, Fold #6: Accuracy: 0.7349, Wall-Time: 49.1951\n",
      "Repeat #0, Fold #7: Accuracy: 0.7396, Wall-Time: 49.3279\n",
      "Repeat #0, Fold #8: Accuracy: 0.7443, Wall-Time: 49.4826\n",
      "Repeat #0, Fold #9: Accuracy: 0.7425, Wall-Time: 48.8083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Simple OpenML Example:\\n')\n",
    "import openml\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Get the task and dataset:\n",
    "#task_id = 146230 # Titanic\n",
    "task_id = 167119 # Jungle Chess\n",
    "task = openml.tasks.get_task(task_id)\n",
    "data = openml.datasets.get_dataset(task.dataset_id)\n",
    "\n",
    "# Specify a classifier and run it:\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "print('Experiment started.')\n",
    "run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n",
    "print('Experiment completed.')\n",
    "print()\n",
    "\n",
    "# Print a summary of the run:\n",
    "print(run)\n",
    "print()\n",
    "\n",
    "# List the available performance metrics:\n",
    "measures = run.fold_evaluations\n",
    "print(\"The timing and performance metrics available: \")\n",
    "for key in measures.keys():\n",
    "    print(key)\n",
    "print()\n",
    "\n",
    "# State the accuracy and runtime of the first repetition and fold:\n",
    "print(\"For the first repetition and fold, accuracy is {:.4f}, training wall time is {:.4f}\".format(measures[\"predictive_accuracy\"][0][0], measures[\"wall_clock_time_millis_training\"][0][0]))\n",
    "print()\n",
    "\n",
    "# Bonus: State the accuracy and runtime on each of the 10 folds:\n",
    "print(\n",
    "    \"The results over all repetitions and folds, \\nwith accuracy (`predictive_accuracy`) \\nand training time(wall_clock_time_millis_training):\"\n",
    ")\n",
    "for repeat, val1 in measures[\"predictive_accuracy\"].items():\n",
    "    for fold, val2 in val1.items():\n",
    "        print(\"Repeat #{}, Fold #{}: Accuracy: {:.4f}, Wall-Time: {:.4f}\".format(repeat, fold, measures[\"predictive_accuracy\"][repeat][fold], measures[\"wall_clock_time_millis_training\"][repeat][fold]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "# Optional: Publish the experiment on OpenML (requires an API key).\n",
    "# For such tests/tutorials, consider using the test server\n",
    "# as to not crowd the main server with runs created by examples.\n",
    "#myrun = run.publish()\n",
    "#print(f\"kNN on {data.name}: {myrun.openml_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrlV3WHw9hKF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimising a model in OpenML\n",
    "\n",
    "A key step in machine learning is performing a systematic hyper-parameter optimization/tuning (HPO) and evaluation.\n",
    "This can easily be done by combining OpenML with the automated HPO tools provided by scikit-learn.\n",
    "\n",
    "**Example-1**: In the examples below, we will use the same task from the simple example above (task `146230` https://www.openml.org/search?type=task&sort=runs&id=146230 for the `Titanic` dataset with did `40704`: https://www.openml.org/search?type=data&sort=runs&id=40704&status=active), but with optimised classifiers. \n",
    "\n",
    "**Example-2 (Bonus)**: the more complex classification task `167119`  https://www.openml.org/search?type=task&sort=runs&id=167119 for the `Jungle Chess` dataset with did `41027`: https://www.openml.org/search?type=data&sort=runs&id=41027&status=active\n",
    "\n",
    "A detailed tutorial on HPO with scikit-learn is available here: https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search\n",
    "A related tutorial for OpenML is given here: https://openml.github.io/openml-python/main/examples/30_extended/fetch_runtimes_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUI4qW-YdFgL"
   },
   "source": [
    "## Example: Run a single model on a task\n",
    "\n",
    "Running a scikit-learn model on a task is done using the function `run_model_on_task(...)` ([see docs](https://openml.github.io/openml-python/master/generated/openml.runs.run_model_on_task.html#openml.runs.run_model_on_task)) or `run_flow_on_task(...)`.  In particular, review the `avoid_duplicate_run` option (especially important for tutorials). The function `get_metric_fn` ([doc](https://openml.github.io/openml-python/master/generated/openml.OpenMLRun.html#openml.OpenMLRun)) can be used to obtain metric scores before uploading. \n",
    "* Use the function `run_model_on_task` to run your favorite scikit-learn classifier (e.g., a `Random Forest Classifier`) on the task `146230` of the `Titanic` dataset. Report the score.\n",
    "* Use the function `run_flow_on_task` to run another scikit-learn classifier on the same task. Report the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5q2zDo3Cftm6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenML Classification Task\n",
      "==========================\n",
      "Task Type Description: https://www.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\n",
      "Task ID..............: 167119\n",
      "Task URL.............: https://www.openml.org/t/167119\n",
      "Estimation Procedure.: crossvalidation\n",
      "Target Feature.......: class\n",
      "# of Classes.........: 3\n",
      "Cost Matrix..........: Available\n",
      "Starting run.\n",
      "Run completed.\n",
      "Repeat #0, Fold #0: ACC 0.773, Training Walltime 298.536\n",
      "Repeat #0, Fold #1: ACC 0.771, Training Walltime 243.038\n",
      "Repeat #0, Fold #2: ACC 0.779, Training Walltime 214.649\n",
      "Repeat #0, Fold #3: ACC 0.784, Training Walltime 349.574\n",
      "Repeat #0, Fold #4: ACC 0.779, Training Walltime 304.194\n",
      "Repeat #0, Fold #5: ACC 0.781, Training Walltime 223.819\n",
      "Repeat #0, Fold #6: ACC 0.766, Training Walltime 223.972\n",
      "Repeat #0, Fold #7: ACC 0.771, Training Walltime 269.892\n",
      "Repeat #0, Fold #8: ACC 0.780, Training Walltime 230.138\n",
      "Repeat #0, Fold #9: ACC 0.777, Training Walltime 204.337\n"
     ]
    }
   ],
   "source": [
    "# Use the function `run_model_on_task` to run your favorite scikit-learn classifier (e.g., a `Random Forest Classifier`) for task 146230 on the `Titanic` dataset.\n",
    "# For starters, do a single run without hyper-parameter tuning:\n",
    "\n",
    "# imports and classifier specification:\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n",
    "\n",
    "# helper function for printing the evaluation results:\n",
    "def print_compare_runs(measures):\n",
    "    for repeat, val1 in measures[\"usercpu_time_millis_training\"].items():\n",
    "        for fold, val2 in val1.items():\n",
    "            print(\n",
    "                \"Repeat #{}, Fold #{}: ACC {:.3f}, Training Walltime {:.3f}\".format(\n",
    "                    repeat, fold, measures[\"predictive_accuracy\"][repeat][fold], measures[\"wall_clock_time_millis_training\"][repeat][fold]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# get the task 146230 and print its summary:\n",
    "#task_id = 146230 # Titanic\n",
    "task_id = 167119 # Jungle Chess\n",
    "task = openml.tasks.get_task(task_id)\n",
    "print(task)\n",
    "\n",
    "# run the configuration and print the results:\n",
    "print('Starting run.')\n",
    "run = openml.runs.run_model_on_task(\n",
    "    model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False,\n",
    ")\n",
    "\n",
    "print('Run completed.')\n",
    "measures = run.fold_evaluations\n",
    "print_compare_runs(measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cq-wXtq_hrtQ"
   },
   "outputs": [],
   "source": [
    "# Use the function run_flow_on_task to run another scikit-learn classifier on the diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8SZV7kqik3o"
   },
   "source": [
    "## Advanced Examples: Random Search and Grid Search\n",
    "\n",
    "Scikit-learn natively supports Random Search and Grid Search procedures, to optimize the hyperparameters. These classifiers can natively be used using the openml connector. Read [this article](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html) to understand how these work. \n",
    "\n",
    "* Run Random Search and Grid Search on a SVM from scikit-learn. Make sure to optimize at least 2 hyperparameters. What are the most important hyperparameters? What is the main difference between these two classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OC8yLpujTaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenML Classification Task\n",
      "==========================\n",
      "Task Type Description: https://www.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\n",
      "Task ID..............: 167119\n",
      "Task URL.............: https://www.openml.org/t/167119\n",
      "Estimation Procedure.: crossvalidation\n",
      "Target Feature.......: class\n",
      "# of Classes.........: 3\n",
      "Cost Matrix..........: Available\n",
      "Starting runs.\n",
      "Runs completed.\n",
      "Repeat #0, Fold #0: ACC 0.775, Training Walltime 1308.550\n",
      "Repeat #0, Fold #1: ACC 0.771, Training Walltime 1301.561\n",
      "Repeat #0, Fold #2: ACC 0.772, Training Walltime 1525.371\n",
      "Repeat #0, Fold #3: ACC 0.787, Training Walltime 1572.490\n",
      "Repeat #0, Fold #4: ACC 0.783, Training Walltime 1542.226\n",
      "Repeat #0, Fold #5: ACC 0.781, Training Walltime 1529.850\n",
      "Repeat #0, Fold #6: ACC 0.766, Training Walltime 1542.290\n",
      "Repeat #0, Fold #7: ACC 0.774, Training Walltime 1553.626\n",
      "Repeat #0, Fold #8: ACC 0.779, Training Walltime 1240.561\n",
      "Repeat #0, Fold #9: ACC 0.777, Training Walltime 1242.521\n"
     ]
    }
   ],
   "source": [
    "# As above, use the function `run_model_on_task` to run your favorite scikit-learn classifier (e.g., a `Random Forest Classifier`) for task 146230 on the `Titanic` dataset.\n",
    "# This time, use a pipe for RandomizedSearchCV:\n",
    "\n",
    "# as above, imports and classifier specification:\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n",
    "\n",
    "# as above, helper function for printing the evaluation results:\n",
    "def print_compare_runs(measures):\n",
    "    for repeat, val1 in measures[\"usercpu_time_millis_training\"].items():\n",
    "        for fold, val2 in val1.items():\n",
    "            print(\n",
    "                \"Repeat #{}, Fold #{}: ACC {:.3f}, Training Walltime {:.3f}\".format(\n",
    "                    repeat, fold, measures[\"predictive_accuracy\"][repeat][fold], measures[\"wall_clock_time_millis_training\"][repeat][fold]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# as above, get the task 146230 and print its summary:\n",
    "#task_id = 146230 # Titanic\n",
    "task_id = 167119 # Jungle Chess\n",
    "task = openml.tasks.get_task(task_id)\n",
    "print(task)\n",
    "\n",
    "# create a pipeline for a RandomizedSearchCV using 2-fold cross validation:\n",
    "n_iter = 2\n",
    "rs_pipe = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions={\n",
    "        \"n_estimators\": np.linspace(start=1, stop=50, num=15).astype(int).tolist()\n",
    "    },\n",
    "    cv=2,\n",
    "    n_iter=n_iter,\n",
    "    n_jobs=2,\n",
    ")\n",
    "\n",
    "\n",
    "# run the search and print the results:\n",
    "print('Starting runs.')\n",
    "run = openml.runs.run_model_on_task(\n",
    "    model=rs_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n",
    ")\n",
    "print('Runs completed.')\n",
    "measures = run.fold_evaluations\n",
    "print_compare_runs(measures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOLdt_UbkxGk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenML Classification Task\n",
      "==========================\n",
      "Task Type Description: https://www.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\n",
      "Task ID..............: 146230\n",
      "Task URL.............: https://www.openml.org/t/146230\n",
      "Estimation Procedure.: crossvalidation\n",
      "Evaluation Measure...: precision\n",
      "Target Feature.......: class\n",
      "# of Classes.........: 2\n",
      "Cost Matrix..........: Available\n",
      "Starting runs.\n",
      "Runs completed.\n",
      "Repeat #0, Fold #0: ACC 0.833, Training Walltime 732.122\n",
      "Repeat #0, Fold #1: ACC 0.809, Training Walltime 744.972\n",
      "Repeat #0, Fold #2: ACC 0.800, Training Walltime 840.389\n",
      "Repeat #0, Fold #3: ACC 0.768, Training Walltime 849.087\n",
      "Repeat #0, Fold #4: ACC 0.786, Training Walltime 599.890\n",
      "Repeat #0, Fold #5: ACC 0.791, Training Walltime 587.464\n",
      "Repeat #0, Fold #6: ACC 0.741, Training Walltime 637.936\n",
      "Repeat #0, Fold #7: ACC 0.800, Training Walltime 637.594\n",
      "Repeat #0, Fold #8: ACC 0.764, Training Walltime 568.018\n",
      "Repeat #0, Fold #9: ACC 0.805, Training Walltime 573.165\n"
     ]
    }
   ],
   "source": [
    "# As above, use the function `run_model_on_task` to run your favorite scikit-learn classifier (e.g., a `Random Forest Classifier`) for task 146230 on the `Titanic` dataset.\n",
    "# This time, use a pipe for GridSearchCV:\n",
    "\n",
    "# as above, imports and classifier specification:\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n",
    "\n",
    "# as above, helper function for printing the evaluation results:\n",
    "def print_compare_runs(measures):\n",
    "    for repeat, val1 in measures[\"usercpu_time_millis_training\"].items():\n",
    "        for fold, val2 in val1.items():\n",
    "            print(\n",
    "                \"Repeat #{}, Fold #{}: ACC {:.3f}, Training Walltime {:.3f}\".format(\n",
    "                    repeat, fold, measures[\"predictive_accuracy\"][repeat][fold], measures[\"wall_clock_time_millis_training\"][repeat][fold]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# as above, get the task 146230 and print its summary:\n",
    "#task_id = 146230 # Titanic\n",
    "task_id = 167119 # Jungle Chess\n",
    "task = openml.tasks.get_task(task_id)\n",
    "print(task)\n",
    "\n",
    "# create a pipeline for a RandomizedSearchCV using 2-fold cross validation:\n",
    "n_iter = 5\n",
    "grid_pipe = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid={\"n_estimators\": np.linspace(start=1, stop=50, num=n_iter).astype(int).tolist()},\n",
    "    cv=2,\n",
    "    n_jobs=2,\n",
    ")\n",
    "\n",
    "\n",
    "# run the search and print the results:\n",
    "print('Starting runs.')\n",
    "run = openml.runs.run_model_on_task(\n",
    "    model=grid_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n",
    ")\n",
    "print('Runs completed.')\n",
    "measures = run.fold_evaluations\n",
    "print_compare_runs(measures)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
